{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b974919-6042-44fe-a1a1-9668773d24da",
   "metadata": {},
   "source": [
    "### GET ACCESS TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aeba5ae-47e8-4260-ad9b-1cdf65d254b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BQDIpBJeJ3XntA79YMI-dos2qnKnakUL4Atei6r8z7pdWkghUCVZvKdV3VwCJdhuv0lLdxO3cAuMGkZK3c8qMQ78v81i8tCIvvNVAy95qBdh-Rc7267AgwbuUErhZY_6yGwsdJkcir4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "CLIENT_SECRET='Your_client_secret'\n",
    "CLIENT_ID='Your_client_id'\n",
    "\n",
    "auth_string = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n",
    "auth_bytes = auth_string.encode(\"utf-8\")\n",
    "auth_base64 = base64.b64encode(auth_bytes).decode(\"utf-8\")\n",
    "\n",
    "url = \"https://accounts.spotify.com/api/token\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Basic {auth_base64}\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"grant_type\": \"client_credentials\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "token = response.json()[\"access_token\"]\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa91838e-4dba-468f-a46c-81094f439310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth header: Basic OTMzNDZiODE4ODJkNGQwODhlYjc4OThmMGJkNDQxOWM6KCcyOGVhYzhkZTc1Y2U0ODk0YmZiZTAzZTZlMjQ2ZmI5OCcsKQ==\n"
     ]
    }
   ],
   "source": [
    "print(\"Auth header:\", headers[\"Authorization\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c2bbf-5fdd-40a6-87af-f81c293b0969",
   "metadata": {},
   "source": [
    "### GET DATA FROM SPOTIFY API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502ca694-6f5f-4da5-8b84-28d9472abc54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Spotify data...\n",
      "✅ Spotify_Data.xlsx created successfully\n"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import time\n",
    "\n",
    "# =========================\n",
    "# AUTH SETUP\n",
    "# =========================\n",
    "sp_oauth = SpotifyOAuth(\n",
    "    CLIENT_SECRET='Your_client_secret'\n",
    "    CLIENT_ID='Your_client_id',\n",
    "    redirect_uri=\"http://localhost:3000\",\n",
    "    scope=\"user-read-recently-played user-library-read playlist-read-private user-top-read\",\n",
    "    cache_path=\".spotify_token.json\"\n",
    ")\n",
    "\n",
    "token_info = sp_oauth.get_cached_token()\n",
    "\n",
    "if not token_info:\n",
    "    auth_url = sp_oauth.get_authorize_url()\n",
    "    print(\"Go to this URL and log in:\\n\", auth_url)\n",
    "    redirected = input(\"Paste FULL redirect URL here:\\n\")\n",
    "    code = parse_qs(urlparse(redirected).query).get(\"code\", [None])[0]\n",
    "    token_info = sp_oauth.get_access_token(code)\n",
    "\n",
    "sp = spotipy.Spotify(auth=token_info[\"access_token\"])\n",
    "\n",
    "# =========================\n",
    "# ARTIST IMAGE CACHE\n",
    "# =========================\n",
    "artist_cache = {}\n",
    "\n",
    "def get_artist_image(artist_id):\n",
    "    if artist_id in artist_cache:\n",
    "        return artist_cache[artist_id]\n",
    "\n",
    "    try:\n",
    "        artist = sp.artist(artist_id)\n",
    "        img = artist[\"images\"][0][\"url\"] if artist[\"images\"] else None\n",
    "        artist_cache[artist_id] = img\n",
    "        return img\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# RECENTLY PLAYED\n",
    "# =========================\n",
    "def get_recently_played(limit=50):\n",
    "    results = sp.current_user_recently_played(limit=limit)\n",
    "    rows = []\n",
    "\n",
    "    for item in results[\"items\"]:\n",
    "        t = item[\"track\"]\n",
    "        rows.append({\n",
    "            \"track_name\": t[\"name\"],\n",
    "            \"artist\": t[\"artists\"][0][\"name\"],\n",
    "            \"album\": t[\"album\"][\"name\"],\n",
    "            \"played_at\": item[\"played_at\"],\n",
    "            \"track_id\": t[\"id\"],\n",
    "            \"song_link\": t[\"external_urls\"][\"spotify\"],\n",
    "            \"album_link\": t[\"album\"][\"external_urls\"][\"spotify\"],\n",
    "            \"album_cover\": t[\"album\"][\"images\"][0][\"url\"] if t[\"album\"][\"images\"] else None,\n",
    "            \"artist_link\": t[\"artists\"][0][\"external_urls\"][\"spotify\"],\n",
    "            \"artist_image\": get_artist_image(t[\"artists\"][0][\"id\"])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# SAVED TRACKS\n",
    "# =========================\n",
    "def get_saved_tracks():\n",
    "    rows, offset = [], 0\n",
    "\n",
    "    while True:\n",
    "        results = sp.current_user_saved_tracks(limit=50, offset=offset)\n",
    "\n",
    "        for item in results[\"items\"]:\n",
    "            t = item[\"track\"]\n",
    "            rows.append({\n",
    "                \"track_name\": t[\"name\"],\n",
    "                \"artist\": t[\"artists\"][0][\"name\"],\n",
    "                \"album\": t[\"album\"][\"name\"],\n",
    "                \"added_at\": item[\"added_at\"],\n",
    "                \"track_id\": t[\"id\"],\n",
    "                \"song_link\": t[\"external_urls\"][\"spotify\"],\n",
    "                \"album_link\": t[\"album\"][\"external_urls\"][\"spotify\"],\n",
    "                \"album_cover\": t[\"album\"][\"images\"][0][\"url\"] if t[\"album\"][\"images\"] else None,\n",
    "                \"artist_link\": t[\"artists\"][0][\"external_urls\"][\"spotify\"],\n",
    "                \"artist_image\": get_artist_image(t[\"artists\"][0][\"id\"])\n",
    "            })\n",
    "\n",
    "        if results[\"next\"] is None:\n",
    "            break\n",
    "\n",
    "        offset += 50\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# PLAYLISTS + SONGS\n",
    "# =========================\n",
    "def get_playlists_with_songs():\n",
    "    playlists = sp.current_user_playlists()[\"items\"]\n",
    "    rows = []\n",
    "\n",
    "    for p in playlists:\n",
    "        playlist_id = p[\"id\"]\n",
    "        playlist_name = p[\"name\"]\n",
    "        offset = 0\n",
    "\n",
    "        while True:\n",
    "            items = sp.playlist_items(playlist_id, offset=offset, limit=100)\n",
    "\n",
    "            for item in items[\"items\"]:\n",
    "                if not item[\"track\"]:\n",
    "                    continue\n",
    "                t = item[\"track\"]\n",
    "\n",
    "                rows.append({\n",
    "                    \"playlist\": playlist_name,\n",
    "                    \"track_name\": t[\"name\"],\n",
    "                    \"artist\": t[\"artists\"][0][\"name\"],\n",
    "                    \"album\": t[\"album\"][\"name\"],\n",
    "                    \"track_id\": t[\"id\"],\n",
    "                    \"song_link\": t[\"external_urls\"][\"spotify\"],\n",
    "                    \"album_link\": t[\"album\"][\"external_urls\"][\"spotify\"],\n",
    "                    \"album_cover\": t[\"album\"][\"images\"][0][\"url\"] if t[\"album\"][\"images\"] else None,\n",
    "                    \"artist_link\": t[\"artists\"][0][\"external_urls\"][\"spotify\"],\n",
    "                    \"artist_image\": get_artist_image(t[\"artists\"][0][\"id\"])\n",
    "                })\n",
    "\n",
    "            if items[\"next\"] is None:\n",
    "                break\n",
    "\n",
    "            offset += 100\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# TOP TRACKS / ARTISTS\n",
    "# =========================\n",
    "def get_top_items(item_type, term):\n",
    "    items = sp.current_user_top_tracks(limit=50, time_range=term) if item_type == \"tracks\" \\\n",
    "        else sp.current_user_top_artists(limit=50, time_range=term)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for t in items[\"items\"]:\n",
    "        if item_type == \"tracks\":\n",
    "            rows.append({\n",
    "                \"track_name\": t[\"name\"],\n",
    "                \"artist\": t[\"artists\"][0][\"name\"],\n",
    "                \"track_id\": t[\"id\"],\n",
    "                \"time_range\": term,\n",
    "                \"song_link\": t[\"external_urls\"][\"spotify\"],\n",
    "                \"album\": t[\"album\"][\"name\"],\n",
    "                \"album_link\": t[\"album\"][\"external_urls\"][\"spotify\"],\n",
    "                \"album_cover\": t[\"album\"][\"images\"][0][\"url\"] if t[\"album\"][\"images\"] else None,\n",
    "                \"artist_link\": t[\"artists\"][0][\"external_urls\"][\"spotify\"],\n",
    "                \"artist_image\": get_artist_image(t[\"artists\"][0][\"id\"])\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"artist\": t[\"name\"],\n",
    "                \"genres\": t[\"genres\"],\n",
    "                \"popularity\": t[\"popularity\"],\n",
    "                \"artist_link\": t[\"external_urls\"][\"spotify\"],\n",
    "                \"artist_image\": get_artist_image(t[\"id\"]),\n",
    "                \"time_range\": term\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# RUN EXTRACTION\n",
    "# =========================\n",
    "print(\"Pulling Spotify data...\")\n",
    "\n",
    "df_recent = get_recently_played()\n",
    "df_saved = get_saved_tracks()\n",
    "df_playlists = get_playlists_with_songs()\n",
    "df_top_short = get_top_items(\"tracks\", \"short_term\")\n",
    "df_top_medium = get_top_items(\"tracks\", \"medium_term\")\n",
    "df_top_long = get_top_items(\"tracks\", \"long_term\")\n",
    "df_top_artists = get_top_items(\"artists\", \"long_term\")\n",
    "\n",
    "# =========================\n",
    "# EXPORT\n",
    "# =========================\n",
    "output_path = r\"C:\\Users\\EMMAN\\Downloads\\Spotify_Data.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    df_recent.to_excel(writer, sheet_name=\"Recently Played\", index=False)\n",
    "    df_saved.to_excel(writer, sheet_name=\"Saved Tracks\", index=False)\n",
    "    df_playlists.to_excel(writer, sheet_name=\"Playlists\", index=False)\n",
    "    df_top_short.to_excel(writer, sheet_name=\"Top Tracks (Short)\", index=False)\n",
    "    df_top_medium.to_excel(writer, sheet_name=\"Top Tracks (Medium)\", index=False)\n",
    "    df_top_long.to_excel(writer, sheet_name=\"Top Tracks (Long)\", index=False)\n",
    "    df_top_artists.to_excel(writer, sheet_name=\"Top Artists\", index=False)\n",
    "\n",
    "print(\"✅ Spotify_Data.xlsx created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cba7d-23c5-4bc4-9a2a-0adba5f37093",
   "metadata": {},
   "source": [
    "### GET ADDITIONAL LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff1d09-1018-4a91-9621-90f62c7bdf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Successfully read file using utf-8 encoding\n",
      "\n",
      "Available columns:\n",
      "['Ts', 'Platform', 'Ms Played', 'Conn Country', 'Ip Addr', 'Master Metadata Track Name', 'Master Metadata Album Artist Name', 'Master Metadata Album Album Name', 'Spotify Track Uri', 'Episode Name', 'Episode Show Name', 'Spotify Episode Uri', 'Audiobook Title', 'Audiobook Uri', 'Audiobook Chapter Uri', 'Audiobook Chapter Title', 'Reason Start', 'Reason End', 'Shuffle', 'Skipped', 'Offline', 'Offline Timestamp', 'Incognito Mode']\n",
      "\n",
      "Processing 1129 unique tracks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████▋          | 981/1129 [07:05<00:59,  2.49it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.cache_handler import MemoryCacheHandler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CLIENT_SECRET='Your_client_secret'\n",
    "CLIENT_ID='Your_client_id'\n",
    "INPUT_EXCEL = r\"C:\\Users\\EMMAN\\Downloads\\Streaming_History_Audio_2023-2025_Streaming_History.csv\" \n",
    "OUTPUT_FILE = r\"C:\\Users\\EMMAN\\Downloads\\Spotify_Enriched_Workbook.xlsx\"\n",
    "\n",
    "# Initialize Spotify API\n",
    "auth_manager = SpotifyClientCredentials(\n",
    "    client_id=CLIENT_ID, \n",
    "    client_secret=CLIENT_SECRET,\n",
    "    cache_handler=MemoryCacheHandler()\n",
    ")\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "def get_spotify_data(series, search_type):\n",
    "    \"\"\"Generic function to search Spotify for unique values in a Series.\"\"\"\n",
    "    # Drop empty rows and get unique names to save API calls\n",
    "    unique_items = series.dropna().unique()\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(unique_items)} unique {search_type}s...\")\n",
    "    \n",
    "    for item_name in tqdm(unique_items):\n",
    "        try:\n",
    "            # Search Spotify\n",
    "            res = sp.search(q=str(item_name), type=search_type, limit=1)\n",
    "            \n",
    "            if search_type == 'track' and res['tracks']['items']:\n",
    "                data = res['tracks']['items'][0]\n",
    "                results.append({\n",
    "                    'Original Name': item_name,\n",
    "                    'Spotify Link': data['external_urls']['spotify'],\n",
    "                    'Image Link': data['album']['images'][0]['url'] if data['album']['images'] else None\n",
    "                })\n",
    "            \n",
    "            elif search_type == 'album' and res['albums']['items']:\n",
    "                data = res['albums']['items'][0]\n",
    "                results.append({\n",
    "                    'Original Name': item_name,\n",
    "                    'Spotify Link': data['external_urls']['spotify'],\n",
    "                    'Image Link': data['images'][0]['url'] if data['images'] else None\n",
    "                })\n",
    "                \n",
    "            elif search_type == 'artist' and res['artists']['items']:\n",
    "                data = res['artists']['items'][0]\n",
    "                results.append({\n",
    "                    'Original Name': item_name,\n",
    "                    'Spotify Link': data['external_urls']['spotify'],\n",
    "                    'Image Link': data['images'][0]['url'] if data['images'] else None\n",
    "                })\n",
    "            else:\n",
    "                results.append({'Original Name': item_name, 'Spotify Link': 'NOT FOUND', 'Image Link': None})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {item_name}: {e}\")\n",
    "            results.append({'Original Name': item_name, 'Spotify Link': 'ERROR', 'Image Link': None})\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT_EXCEL):\n",
    "        print(f\"Error: Could not find the file at {INPUT_EXCEL}.\")\n",
    "        return\n",
    "\n",
    "    # 1. Read the CSV file with automatic encoding detection\n",
    "    print(\"Reading CSV file...\")\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']\n",
    "    \n",
    "    df = None\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(INPUT_EXCEL, encoding=encoding)\n",
    "            print(f\"Successfully read file using {encoding} encoding\")\n",
    "            break\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Error: Could not read file with any common encoding\")\n",
    "        return\n",
    "    \n",
    "    # 2. Print available columns to help debug\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    print(df.columns.tolist())\n",
    "    print()\n",
    "\n",
    "    # 3. Verify columns exist before processing\n",
    "    # Common Spotify history column names (adjust based on your actual columns)\n",
    "    possible_track_cols = ['master_metadata_track_name', 'Master Metadata Track Name', 'track_name']\n",
    "    possible_album_cols = ['master_metadata_album_album_name', 'Master Metadata Album Album Name', 'album_name']\n",
    "    possible_artist_cols = ['master_metadata_album_artist_name', 'Master Metadata Album Artist Name', 'artist_name']\n",
    "    \n",
    "    # Find the actual column names\n",
    "    track_col = None\n",
    "    album_col = None\n",
    "    artist_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.lower() in [c.lower() for c in possible_track_cols]:\n",
    "            track_col = col\n",
    "        elif col.lower() in [c.lower() for c in possible_album_cols]:\n",
    "            album_col = col\n",
    "        elif col.lower() in [c.lower() for c in possible_artist_cols]:\n",
    "            artist_col = col\n",
    "    \n",
    "    if not track_col or not album_col or not artist_col:\n",
    "        print(\"Error: Could not find required columns.\")\n",
    "        print(f\"Looking for track column (found: {track_col})\")\n",
    "        print(f\"Looking for album column (found: {album_col})\")\n",
    "        print(f\"Looking for artist column (found: {artist_col})\")\n",
    "        return\n",
    "\n",
    "    # 4. Process each column specifically\n",
    "    tracks_df = get_spotify_data(df[track_col], 'track')\n",
    "    albums_df = get_spotify_data(df[album_col], 'album')\n",
    "    artists_df = get_spotify_data(df[artist_col], 'artist')\n",
    "\n",
    "    # 5. Save to a single Excel Workbook with three sheets\n",
    "    print(f\"\\nSaving enriched data to {OUTPUT_FILE}...\")\n",
    "    with pd.ExcelWriter(OUTPUT_FILE, engine='xlsxwriter') as writer:\n",
    "        tracks_df.to_excel(writer, sheet_name='Tracks', index=False)\n",
    "        albums_df.to_excel(writer, sheet_name='Albums', index=False)\n",
    "        artists_df.to_excel(writer, sheet_name='Artists', index=False)\n",
    "\n",
    "    print(f\"\\nSuccess! File saved at: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7a7ef-ca7c-430e-a55e-56aee1c99eee",
   "metadata": {},
   "source": [
    "### DOWNLOAD THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39e2d7c-8fbc-44b3-9596-6b2de64097f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING ARTISTS\n",
      "============================================================\n",
      "Found 298 missing items with valid URLs. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Artist: 100%|████████████████████████████████████████████████████████████| 298/298 [05:18<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully downloaded: 298\n",
      "❌ Failed: 0\n",
      "Total .jpg files now: 297\n",
      "\n",
      "============================================================\n",
      "PROCESSING ALBUMS\n",
      "============================================================\n",
      "Found 806 missing items with valid URLs. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Album: 100%|█████████████████████████████████████████████████████████████| 806/806 [12:02<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully downloaded: 806\n",
      "❌ Failed: 0\n",
      "Total .jpg files now: 802\n",
      "\n",
      "============================================================\n",
      "PROCESSING TRACKS\n",
      "============================================================\n",
      "Found 1130 missing items with valid URLs. Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Track: 100%|███████████████████████████████████████████████████████████| 1130/1130 [18:11<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully downloaded: 1130\n",
      "❌ Failed: 0\n",
      "Total .jpg files now: 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_WORKBOOK = r\"C:\\Users\\EMMAN\\Downloads\\Spotify_Enriched_Workbook.xlsx\"\n",
    "FOLDERS = {\n",
    "    'Artist': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Artist Image\",\n",
    "    'Album': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Album covers\",\n",
    "    'Track': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Tracks covers\"\n",
    "}\n",
    "\n",
    "for folder in FOLDERS.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Removes characters that are illegal in Windows filenames.\"\"\"\n",
    "    if not isinstance(filename, str) or filename.lower() == 'nan':\n",
    "        return \"Unknown\"\n",
    "    clean = re.sub(r'[<>:\"/\\\\|?*]', '', filename).strip()\n",
    "    return clean[:150]\n",
    "\n",
    "def download_and_resize_image(url, folder, filename, size=(640, 640), max_retries=3):\n",
    "    \"\"\"Downloads, resizes to specific px, converts to RGB, and saves as .jpg with retries\"\"\"\n",
    "    if not url or pd.isna(url) or str(url).lower() in ['none', 'not found', 'error']:\n",
    "        return False, \"No valid URL\"\n",
    "    \n",
    "    filepath = os.path.join(folder, f\"{filename}.jpg\")\n",
    "    \n",
    "    # Double check if file exists\n",
    "    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "        return True, \"Already exists\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            \n",
    "            if response.status_code == 200 and len(response.content) > 0:\n",
    "                # Load image from the response content\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                \n",
    "                # Convert to RGB (required for saving as JPEG)\n",
    "                if img.mode != \"RGB\":\n",
    "                    img = img.convert(\"RGB\")\n",
    "                \n",
    "                # Resize using Lanczos filter for high quality\n",
    "                img_resized = img.resize(size, Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Save as JPEG with high quality\n",
    "                img_resized.save(filepath, \"JPEG\", quality=95, optimize=True)\n",
    "                \n",
    "                # Verify file was actually saved\n",
    "                if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "                    return True, \"Downloaded successfully\"\n",
    "                else:\n",
    "                    return False, \"File not saved properly\"\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1)  # Wait before retry\n",
    "                    continue\n",
    "                return False, f\"HTTP {response.status_code}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Wait before retry\n",
    "                continue\n",
    "            return False, str(e)\n",
    "    \n",
    "    return False, \"Max retries exceeded\"\n",
    "\n",
    "def download_missing_only():\n",
    "    \"\"\"Only download items that are missing from folders\"\"\"\n",
    "    \n",
    "    for sheet_name, folder_key in [('Artists', 'Artist'), ('Albums', 'Album'), ('Tracks', 'Track')]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING {sheet_name.upper()}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        df = pd.read_excel(INPUT_WORKBOOK, sheet_name=sheet_name)\n",
    "        folder = FOLDERS[folder_key]\n",
    "        \n",
    "        # Get list of actual files in folder\n",
    "        actual_files = set(f.replace('.jpg', '') for f in os.listdir(folder) if f.endswith('.jpg'))\n",
    "        \n",
    "        # Find missing items\n",
    "        missing = []\n",
    "        for _, row in df.iterrows():\n",
    "            original_name = str(row['Original Name'])\n",
    "            sanitized_name = sanitize_filename(original_name)\n",
    "            \n",
    "            if sanitized_name not in actual_files:\n",
    "                url = row['Image Link']\n",
    "                has_url = not (pd.isna(url) or str(url).lower() in ['none', 'not found', 'error', 'nan'])\n",
    "                if has_url:  # Only add if it has a valid URL\n",
    "                    missing.append({\n",
    "                        'name': sanitized_name,\n",
    "                        'url': url\n",
    "                    })\n",
    "        \n",
    "        if not missing:\n",
    "            print(f\"✓ All images present! ({len(actual_files)} files)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(missing)} missing items with valid URLs. Downloading...\")\n",
    "        \n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for item in tqdm(missing, desc=f\"Downloading {folder_key}\"):\n",
    "            success, reason = download_and_resize_image(item['url'], folder, item['name'])\n",
    "            if success:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"\\n  ❌ Failed: {item['name']} - {reason}\")\n",
    "        \n",
    "        print(f\"\\n✓ Successfully downloaded: {success_count}\")\n",
    "        print(f\"❌ Failed: {failed_count}\")\n",
    "        \n",
    "        # Final count\n",
    "        final_files = len([f for f in os.listdir(folder) if f.endswith('.jpg')])\n",
    "        print(f\"Total .jpg files now: {final_files}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_missing_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0dd4e-86fb-429f-bbb6-7526db724151",
   "metadata": {},
   "source": [
    "### KNOW WHICH IMAGES ARE MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff6638f-42d7-447c-9e71-7de2b8d36f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHECKING ARTISTS\n",
      "============================================================\n",
      "\n",
      "❌ 3 items missing from folder:\n",
      "\n",
      "   Original: 'Iyanu'\n",
      "   Sanitized: 'Iyanu'\n",
      "   ❌ No URL: No URL\n",
      "\n",
      "   Original: 'The Gospel Messages'\n",
      "   Sanitized: 'The Gospel Messages'\n",
      "   ❌ No URL: No URL\n",
      "\n",
      "   Original: 'Bbo'\n",
      "   Sanitized: 'Bbo'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab6761610000e5eb27bb0b12d0e8a42e50a13c34\n",
      "\n",
      "Expected: 300 | Found: 297 | Missing: 3\n",
      "\n",
      "============================================================\n",
      "CHECKING ALBUMS\n",
      "============================================================\n",
      "\n",
      "❌ 4 items missing from folder:\n",
      "\n",
      "   Original: 'You Are Great'\n",
      "   Sanitized: 'You Are Great'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273588a126005750b198d90ebdf\n",
      "\n",
      "   Original: 'Alignment Chant'\n",
      "   Sanitized: 'Alignment Chant'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273f823f8e0f81c2e20d84d65f5\n",
      "\n",
      "   Original: 'ELOHIM ADONAI'\n",
      "   Sanitized: 'ELOHIM ADONAI'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b2736a4bec5b9efbf3fd519a7f69\n",
      "\n",
      "   Original: 'Fathered By The Best'\n",
      "   Sanitized: 'Fathered By The Best'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273fc7b0e1b426a3a615b556705\n",
      "\n",
      "Expected: 806 | Found: 802 | Missing: 4\n",
      "\n",
      "============================================================\n",
      "CHECKING TRACKS\n",
      "============================================================\n",
      "\n",
      "❌ 12 items missing from folder:\n",
      "\n",
      "   Original: 'You Are Great'\n",
      "   Sanitized: 'You Are Great'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273c3ee53191d7c42db4739662c\n",
      "\n",
      "   Original: 'Burn'\n",
      "   Sanitized: 'Burn'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b2735b069390c128a4b4c3197d80\n",
      "\n",
      "   Original: 'Sound Of Revival'\n",
      "   Sanitized: 'Sound Of Revival'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273880f6c3c55fa44bb53ce8482\n",
      "\n",
      "   Original: 'Agbara Olorun po'\n",
      "   Sanitized: 'Agbara Olorun po'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b2737660d8605b9f287bd09f2799\n",
      "\n",
      "   Original: 'PRAISE MEDLEY'\n",
      "   Sanitized: 'PRAISE MEDLEY'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b27396fdd4431706c10809c4a7c6\n",
      "\n",
      "   Original: 'GRATITUDE'\n",
      "   Sanitized: 'GRATITUDE'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b27331b046b5e8493d36db0f11da\n",
      "\n",
      "   Original: 'JESUS'\n",
      "   Sanitized: 'JESUS'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b2736bf7f7fed9d5d9f18944ef8b\n",
      "\n",
      "   Original: 'El Elyon'\n",
      "   Sanitized: 'El Elyon'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273ae8ed4e06ee0ea8e5caac55b\n",
      "\n",
      "   Original: 'I'm the One'\n",
      "   Sanitized: 'I'm the One'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273f2b55cfff049e03f92834b50\n",
      "\n",
      "   Original: 'ELOHIM ADONAI'\n",
      "   Sanitized: 'ELOHIM ADONAI'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b2736a4bec5b9efbf3fd519a7f69\n",
      "\n",
      "   Original: 'You Are Yahweh'\n",
      "   Sanitized: 'You Are Yahweh'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273e96ee306af6cd2e4e38556a1\n",
      "\n",
      "   Original: 'For Life'\n",
      "   Sanitized: 'For Life'\n",
      "   ✓ Has URL: https://i.scdn.co/image/ab67616d0000b273e351f7979afa1bde6c70302b\n",
      "\n",
      "Expected: 1130 | Found: 1118 | Missing: 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "INPUT_WORKBOOK = r\"C:\\Users\\EMMAN\\Downloads\\Spotify_Enriched_Workbook.xlsx\"\n",
    "FOLDERS = {\n",
    "    'Artist': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Artist Image\",\n",
    "    'Album': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Album covers\",\n",
    "    'Track': r\"C:\\Users\\EMMAN\\OneDrive\\Documents\\My Tableau Repository\\Shapes\\Tracks covers\"\n",
    "}\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Removes characters that are illegal in Windows filenames.\"\"\"\n",
    "    if not isinstance(filename, str) or filename.lower() == 'nan':\n",
    "        return \"Unknown\"\n",
    "    clean = re.sub(r'[<>:\"/\\\\|?*]', '', filename).strip()\n",
    "    return clean[:150]\n",
    "\n",
    "def find_missing_images():\n",
    "    for sheet_name, folder_key in [('Artists', 'Artist'), ('Albums', 'Album'), ('Tracks', 'Track')]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CHECKING {sheet_name.upper()}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        df = pd.read_excel(INPUT_WORKBOOK, sheet_name=sheet_name)\n",
    "        folder = FOLDERS[folder_key]\n",
    "        \n",
    "        # Get list of actual files in folder\n",
    "        actual_files = set(f.replace('.jpg', '') for f in os.listdir(folder) if f.endswith('.jpg'))\n",
    "        \n",
    "        missing = []\n",
    "        for _, row in df.iterrows():\n",
    "            original_name = str(row['Original Name'])\n",
    "            sanitized_name = sanitize_filename(original_name)\n",
    "            \n",
    "            if sanitized_name not in actual_files:\n",
    "                url = row['Image Link']\n",
    "                has_url = not (pd.isna(url) or str(url).lower() in ['none', 'not found', 'error', 'nan'])\n",
    "                missing.append({\n",
    "                    'Original': original_name,\n",
    "                    'Sanitized': sanitized_name,\n",
    "                    'Has URL': has_url,\n",
    "                    'URL': url if has_url else 'No URL'\n",
    "                })\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"\\n❌ {len(missing)} items missing from folder:\")\n",
    "            for item in missing:\n",
    "                status = \"✓ Has URL\" if item['Has URL'] else \"❌ No URL\"\n",
    "                print(f\"\\n   Original: '{item['Original']}'\")\n",
    "                print(f\"   Sanitized: '{item['Sanitized']}'\")\n",
    "                print(f\"   {status}: {item['URL']}\")\n",
    "        else:\n",
    "            print(f\"✓ All {len(df)} images present in folder\")\n",
    "        \n",
    "        print(f\"\\nExpected: {len(df)} | Found: {len(actual_files)} | Missing: {len(missing)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_missing_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8e608-eeb0-4e05-bd0f-ef6fcc2080e2",
   "metadata": {},
   "source": [
    "### CONVERT JSON TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52595300-afeb-4af6-ae10-3a9e9351f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = r\"C:\\Users\\EMMAN\\Downloads\\my_spotify_data\\Spotify Extended Streaming History\\Streaming_History_Audio_2023-2026.json\"\n",
    "output_csv = r\"C:\\Users\\EMMAN\\Downloads\\Streaming_History_Audio_2023-2026.csv\"\n",
    "\n",
    "df = pd.read_json(json_file)\n",
    "\n",
    "# Convert timestamp\n",
    "if 'ts' in df.columns:\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.replace(\"_\", \" \").str.title()\n",
    "\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a20e6-4a28-49f2-bf70-49332c524a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efab23e-29c4-481d-b19f-8e32becff529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
